#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
í™˜ ë¦¬ìŠ¤í¬ í—·ì§€ë¥¼ ìœ„í•œ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸
PPO ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Any
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class HedgeActorCritic(nn.Module):
    """í—·ì§€ ì „ëµì„ ìœ„í•œ Actor-Critic ì‹ ê²½ë§"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(HedgeActorCritic, self).__init__()
        
        # ê³µí†µ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Actor (ì •ì±…) ë„¤íŠ¸ì›Œí¬
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # Critic (ê°€ì¹˜) ë„¤íŠ¸ì›Œí¬
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            torch.nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
            module.bias.data.zero_()
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        features = self.feature_extractor(state)
        action_probs = F.softmax(self.actor(features), dim=-1)
        value = self.critic(features)
        return action_probs, value
    
    def get_action_and_value(self, state: torch.Tensor, action: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """ì•¡ì…˜ê³¼ ê°€ì¹˜ ë°˜í™˜"""
        action_probs, value = self.forward(state)
        dist = Categorical(action_probs)
        
        if action is None:
            action = dist.sample()
        
        log_prob = dist.log_prob(action)
        entropy = dist.entropy()
        
        return action, log_prob, entropy, value

class HedgeAgent:
    """í™˜ ë¦¬ìŠ¤í¬ í—·ì§€ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸"""
    
    def __init__(self, 
                 state_dim: int,
                 action_dim: int,
                 learning_rate: float = 3e-4,
                 gamma: float = 0.99,
                 gae_lambda: float = 0.95,
                 clip_ratio: float = 0.2,
                 value_loss_coef: float = 0.5,
                 entropy_coef: float = 0.01,
                 max_grad_norm: float = 0.5,
                 device: str = 'auto'):
        """
        Args:
            state_dim: ìƒíƒœ ì°¨ì›
            action_dim: ì•¡ì…˜ ì°¨ì›
            learning_rate: í•™ìŠµë¥ 
            gamma: í• ì¸ ê³„ìˆ˜
            gae_lambda: GAE ëŒë‹¤
            clip_ratio: PPO í´ë¦½ ë¹„ìœ¨
            value_loss_coef: ê°€ì¹˜ ì†ì‹¤ ê³„ìˆ˜
            entropy_coef: ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
            max_grad_norm: ìµœëŒ€ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„
            device: ë””ë°”ì´ìŠ¤
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = HedgeActorCritic(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
        # í•™ìŠµ íˆìŠ¤í† ë¦¬
        self.training_history = {
            'episode_rewards': [],
            'episode_lengths': [],
            'actor_losses': [],
            'critic_losses': [],
            'entropy_losses': [],
            'total_losses': []
        }
    
    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """ì•¡ì…˜ ì„ íƒ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            action_probs, _ = self.model(state_tensor)
            
            if training:
                # í•™ìŠµì‹œì—ëŠ” í™•ë¥ ì  ì•¡ì…˜ ì„ íƒ
                dist = Categorical(action_probs)
                action = dist.sample()
            else:
                # í‰ê°€ì‹œì—ëŠ” ìµœì  ì•¡ì…˜ ì„ íƒ
                action = torch.argmax(action_probs, dim=-1)
        
        return action.item()
    
    def compute_gae(self, rewards: List[float], values: List[float], dones: List[bool]) -> List[float]:
        """Generalized Advantage Estimation (GAE) ê³„ì‚°"""
        advantages = []
        gae = 0
        
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1]
            
            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
            advantages.insert(0, gae)
        
        return advantages
    
    def update(self, states: List[np.ndarray], actions: List[int], 
               rewards: List[float], dones: List[bool], 
               log_probs: List[torch.Tensor]) -> Dict[str, float]:
        """ëª¨ë¸ ì—…ë°ì´íŠ¸ (PPO)"""
        # GAE ê³„ì‚°
        with torch.no_grad():
            states_tensor = torch.FloatTensor(states).to(self.device)
            _, values = self.model(states_tensor)
            values = values.squeeze().cpu().numpy()
        
        advantages = self.compute_gae(rewards, values, dones)
        returns = np.array(advantages) + np.array(values)
        
        # ì •ê·œí™”
        advantages = (np.array(advantages) - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        # í…ì„œ ë³€í™˜
        states_tensor = torch.FloatTensor(states).to(self.device)
        actions_tensor = torch.LongTensor(actions).to(self.device)
        advantages_tensor = torch.FloatTensor(advantages).to(self.device)
        returns_tensor = torch.FloatTensor(returns).to(self.device)
        old_log_probs_tensor = torch.stack(log_probs).detach()
        
        # PPO ì—…ë°ì´íŠ¸
        for _ in range(10):  # ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸
            action_probs, values = self.model(states_tensor)
            dist = Categorical(action_probs)
            new_log_probs = dist.log_prob(actions_tensor)
            entropy = dist.entropy()
            
            # ë¹„ìœ¨ ê³„ì‚°
            ratio = torch.exp(new_log_probs - old_log_probs_tensor)
            
            # í´ë¦½ëœ ëª©ì  í•¨ìˆ˜
            surr1 = ratio * advantages_tensor
            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages_tensor
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # ê°€ì¹˜ ì†ì‹¤
            value_loss = F.mse_loss(values.squeeze(), returns_tensor)
            
            # ì—”íŠ¸ë¡œí”¼ ì†ì‹¤
            entropy_loss = -entropy.mean()
            
            # ì „ì²´ ì†ì‹¤
            total_loss = (actor_loss + 
                         self.value_loss_coef * value_loss + 
                         self.entropy_coef * entropy_loss)
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
            self.optimizer.step()
        
        # ì†ì‹¤ ê¸°ë¡
        losses = {
            'actor_loss': actor_loss.item(),
            'critic_loss': value_loss.item(),
            'entropy_loss': entropy_loss.item(),
            'total_loss': total_loss.item()
        }
        
        return losses
    
    def train_episode(self, env) -> Tuple[float, int, Dict[str, float]]:
        """ì—í”¼ì†Œë“œ í•™ìŠµ"""
        state, info = env.reset()
        episode_reward = 0
        episode_length = 0
        
        # ì—í”¼ì†Œë“œ ë°ì´í„° ìˆ˜ì§‘
        states, actions, rewards, dones, log_probs = [], [], [], [], []
        
        while True:
            # ì•¡ì…˜ ì„ íƒ
            action = self.select_action(state, training=True)
            
            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_state, reward, done, truncated, info = env.step(action)
            
            # ë°ì´í„° ì €ì¥
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            dones.append(done)
            
            # ë¡œê·¸ í™•ë¥  ê³„ì‚°
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            action_tensor = torch.LongTensor([action]).to(self.device)
            with torch.no_grad():
                _, log_prob, _, _ = self.model.get_action_and_value(state_tensor, action_tensor)
            log_probs.append(log_prob.squeeze())
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state = next_state
            episode_reward += reward
            episode_length += 1
            
            if done or truncated:
                break
        
        # ëª¨ë¸ ì—…ë°ì´íŠ¸
        losses = self.update(states, actions, rewards, dones, log_probs)
        
        return episode_reward, episode_length, losses
    
    def train(self, env, num_episodes: int = 1000, eval_interval: int = 100) -> Dict[str, List]:
        """í•™ìŠµ ì‹¤í–‰"""
        print(f"ğŸ¯ {num_episodes} ì—í”¼ì†Œë“œ í•™ìŠµ ì‹œì‘...")
        
        for episode in tqdm(range(num_episodes), desc="í•™ìŠµ ì§„í–‰ë¥ "):
            # ì—í”¼ì†Œë“œ í•™ìŠµ
            episode_reward, episode_length, losses = self.train_episode(env)
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            self.training_history['episode_rewards'].append(episode_reward)
            self.training_history['episode_lengths'].append(episode_length)
            self.training_history['actor_losses'].append(losses['actor_loss'])
            self.training_history['critic_losses'].append(losses['critic_loss'])
            self.training_history['entropy_losses'].append(losses['entropy_loss'])
            self.training_history['total_losses'].append(losses['total_loss'])
            
            # í‰ê°€ ë° ì¶œë ¥
            if (episode + 1) % eval_interval == 0:
                avg_reward = np.mean(self.training_history['episode_rewards'][-eval_interval:])
                avg_length = np.mean(self.training_history['episode_lengths'][-eval_interval:])
                avg_loss = np.mean(self.training_history['total_losses'][-eval_interval:])
                
                print(f"ğŸ“Š ì—í”¼ì†Œë“œ {episode + 1}: "
                      f"í‰ê·  ë³´ìƒ: {avg_reward:.2f}, "
                      f"í‰ê·  ê¸¸ì´: {avg_length:.1f}, "
                      f"í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        return self.training_history
    
    def evaluate(self, env, num_episodes: int = 100) -> Dict[str, float]:
        """ì—ì´ì „íŠ¸ í‰ê°€"""
        print(f"ğŸ” {num_episodes} ì—í”¼ì†Œë“œ í‰ê°€ ì¤‘...")
        
        episode_rewards = []
        episode_lengths = []
        final_capitals = []
        
        for episode in range(num_episodes):
            state, info = env.reset()
            episode_reward = 0
            episode_length = 0
            
            while True:
                # í‰ê°€ì‹œì—ëŠ” ìµœì  ì•¡ì…˜ ì„ íƒ
                action = self.select_action(state, training=False)
                next_state, reward, done, truncated, info = env.step(action)
                
                state = next_state
                episode_reward += reward
                episode_length += 1
                
                if done or truncated:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            final_capitals.append(info['current_capital'])
        
        # í‰ê°€ ê²°ê³¼
        evaluation_results = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'mean_final_capital': np.mean(final_capitals),
            'std_final_capital': np.std(final_capitals),
            'sharpe_ratio': np.mean(episode_rewards) / (np.std(episode_rewards) + 1e-8)
        }
        
        print(f"ğŸ“ˆ í‰ê°€ ê²°ê³¼:")
        print(f"   í‰ê·  ë³´ìƒ: {evaluation_results['mean_reward']:.2f} Â± {evaluation_results['std_reward']:.2f}")
        print(f"   í‰ê·  ê¸¸ì´: {evaluation_results['mean_length']:.1f}")
        print(f"   í‰ê·  ìµœì¢… ìë³¸: ${evaluation_results['mean_final_capital']:,.2f}")
        print(f"   ìƒ¤í”„ ë¹„ìœ¨: {evaluation_results['sharpe_ratio']:.4f}")
        
        return evaluation_results
    
    def save_model(self, filepath: str) -> None:
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'training_history': self.training_history
        }, filepath)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    def load_model(self, filepath: str) -> None:
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.training_history = checkpoint['training_history']
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")
    
    def plot_training_history(self, save_path: str = None) -> None:
        """í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ì—í”¼ì†Œë“œ ë³´ìƒ
        axes[0, 0].plot(self.training_history['episode_rewards'])
        axes[0, 0].set_title('ì—í”¼ì†Œë“œ ë³´ìƒ')
        axes[0, 0].set_xlabel('ì—í”¼ì†Œë“œ')
        axes[0, 0].set_ylabel('ë³´ìƒ')
        axes[0, 0].grid(True, alpha=0.3)
        
        # ì—í”¼ì†Œë“œ ê¸¸ì´
        axes[0, 1].plot(self.training_history['episode_lengths'])
        axes[0, 1].set_title('ì—í”¼ì†Œë“œ ê¸¸ì´')
        axes[0, 1].set_xlabel('ì—í”¼ì†Œë“œ')
        axes[0, 1].set_ylabel('ê¸¸ì´')
        axes[0, 1].grid(True, alpha=0.3)
        
        # ì†ì‹¤ í•¨ìˆ˜ë“¤
        axes[1, 0].plot(self.training_history['actor_losses'], label='Actor Loss', alpha=0.7)
        axes[1, 0].plot(self.training_history['critic_losses'], label='Critic Loss', alpha=0.7)
        axes[1, 0].plot(self.training_history['entropy_losses'], label='Entropy Loss', alpha=0.7)
        axes[1, 0].set_title('ì†ì‹¤ í•¨ìˆ˜')
        axes[1, 0].set_xlabel('ì—í”¼ì†Œë“œ')
        axes[1, 0].set_ylabel('ì†ì‹¤')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # ì „ì²´ ì†ì‹¤
        axes[1, 1].plot(self.training_history['total_losses'])
        axes[1, 1].set_title('ì „ì²´ ì†ì‹¤')
        axes[1, 1].set_xlabel('ì—í”¼ì†Œë“œ')
        axes[1, 1].set_ylabel('ì†ì‹¤')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥: {save_path}")
        
        plt.show()
