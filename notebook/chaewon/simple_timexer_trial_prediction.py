#!/usr/bin/env python3
"""
df_trial.csvë¡œ ê°„ë‹¨í•œ TimeXer ìŠ¤íƒ€ì¼ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  target_trial.csvë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
TimeXerì˜ í•µì‹¬ ì•„ì´ë””ì–´: ë‚´ìƒë³€ìˆ˜ì™€ ì™¸ìƒë³€ìˆ˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# 1. ê°„ë‹¨í•œ TimeXer ìŠ¤íƒ€ì¼ ëª¨ë¸ êµ¬í˜„
# ============================================================================

class SimpleTimeXerModel(nn.Module):
    """
    TimeXerì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì ìš©í•œ ê°„ë‹¨í•œ ëª¨ë¸:
    - ë‚´ìƒë³€ìˆ˜(usdkrw): LSTMìœ¼ë¡œ ì‹œê°„ì  íŒ¨í„´ í•™ìŠµ
    - ì™¸ìƒë³€ìˆ˜: ë³„ë„ ì²˜ë¦¬ í›„ ê²°í•©
    - ê¸€ë¡œë²Œ í† í°: ë‘ ì •ë³´ë¥¼ ì—°ê²°í•˜ëŠ” ë¸Œë¦¿ì§€
    """
    
    def __init__(self, endogenous_dim=1, exogenous_dim=5, hidden_size=128, num_layers=2, output_size=7):
        super(SimpleTimeXerModel, self).__init__()
        
        self.endogenous_dim = endogenous_dim
        self.exogenous_dim = exogenous_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        
        # ë‚´ìƒë³€ìˆ˜ ì²˜ë¦¬ (usdkrw)
        self.endogenous_lstm = nn.LSTM(
            input_size=endogenous_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.1
        )
        
        # ì™¸ìƒë³€ìˆ˜ ì²˜ë¦¬
        self.exogenous_lstm = nn.LSTM(
            input_size=exogenous_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.1
        )
        
        # ê¸€ë¡œë²Œ í† í° (TimeXerì˜ í•µì‹¬ ì•„ì´ë””ì–´)
        self.global_token = nn.Parameter(torch.randn(1, hidden_size))
        
        # êµì°¨ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ (Cross-Attention)
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, output_size)
        )
        
    def forward(self, endogenous_x, exogenous_x):
        """
        Args:
            endogenous_x: [batch_size, seq_len, 1] - usdkrw ë°ì´í„°
            exogenous_x: [batch_size, seq_len, 5] - ì™¸ìƒë³€ìˆ˜ ë°ì´í„°
        """
        batch_size = endogenous_x.size(0)
        
        # 1. ë‚´ìƒë³€ìˆ˜ ì²˜ë¦¬ (TimeXerì˜ EnEmbedding ì—­í• )
        endogenous_out, (endogenous_h, _) = self.endogenous_lstm(endogenous_x)
        endogenous_features = endogenous_out[:, -1, :]  # ë§ˆì§€ë§‰ ì‹œì ì˜ íŠ¹ì§•
        
        # 2. ì™¸ìƒë³€ìˆ˜ ì²˜ë¦¬ (TimeXerì˜ DataEmbedding_inverted ì—­í• )
        exogenous_out, (exogenous_h, _) = self.exogenous_lstm(exogenous_x)
        exogenous_features = exogenous_out[:, -1, :]  # ë§ˆì§€ë§‰ ì‹œì ì˜ íŠ¹ì§•
        
        # 3. ê¸€ë¡œë²Œ í† í° ì ìš© (TimeXerì˜ í•µì‹¬ ì•„ì´ë””ì–´)
        global_tokens = self.global_token.repeat(batch_size, 1)
        
        # 4. êµì°¨ ì£¼ì˜ë¥¼ í†µí•œ ì •ë³´ ê²°í•© (TimeXerì˜ Cross-Attention ì—­í• )
        # endogenous_featuresë¥¼ queryë¡œ, exogenous_featuresë¥¼ key, valueë¡œ ì‚¬ìš©
        cross_features, _ = self.cross_attention(
            query=endogenous_features.unsqueeze(1),
            key=exogenous_features.unsqueeze(1),
            value=exogenous_features.unsqueeze(1)
        )
        cross_features = cross_features.squeeze(1)
        
        # 5. ê¸€ë¡œë²Œ í† í°ê³¼ ê²°í•©
        combined_features = torch.cat([cross_features, global_tokens], dim=1)
        
        # 6. ìµœì¢… ì¶œë ¥
        output = self.output_layer(combined_features)
        
        return output

# ============================================================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ ìƒì„±
# ============================================================================

def load_and_preprocess_data():
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # í•™ìŠµ ë°ì´í„° ë¡œë“œ
    df_train = pd.read_csv('df_trial.csv')
    df_train['date'] = pd.to_datetime(df_train['date'])
    df_train = df_train.sort_values('date').reset_index(drop=True)
    
    # ì˜ˆì¸¡ ëŒ€ìƒ ë°ì´í„° ë¡œë“œ
    df_target = pd.read_csv('target_trial.csv')
    df_target['date'] = pd.to_datetime(df_target['date'])
    df_target = df_target.sort_values('date').reset_index(drop=True)
    
    print(f"í•™ìŠµ ë°ì´í„°: {df_train.shape} (2023-01-01 ~ 2024-12-31)")
    print(f"ì˜ˆì¸¡ ëŒ€ìƒ: {df_target.shape} (2025-01-01 ~ 2025-01-31)")
    
    # ì™¸ìƒë³€ìˆ˜ ì¤‘ ë³€ë™ì´ ì ì€ ê²ƒë“¤ ì œê±° (ì›”ë³„ ë°ì´í„°)
    # ë³€ë™ì´ ì ì€ ì»¬ëŸ¼ë“¤: us_ex, us_im, reserve, us_reserve, us_export, us_import, us_gdp, us_stock
    # ë³€ë™ì´ ìˆëŠ” ì»¬ëŸ¼ë“¤: base, market, consumer, exp_rate, im_rate
    static_columns = ['us_ex', 'us_im', 'reserve', 'us_reserve', 'us_export', 'us_import', 'us_gdp', 'us_stock']
    dynamic_columns = ['base', 'market', 'consumer', 'exp_rate', 'im_rate']
    
    print(f"ì œê±°í•  ì •ì  ì™¸ìƒë³€ìˆ˜: {static_columns}")
    print(f"ì‚¬ìš©í•  ë™ì  ì™¸ìƒë³€ìˆ˜: {dynamic_columns}")
    
    # ìµœì¢… ì‚¬ìš©í•  ì»¬ëŸ¼ë“¤
    feature_columns = ['usdkrw(target)'] + dynamic_columns
    print(f"ìµœì¢… íŠ¹ì„± ì»¬ëŸ¼: {feature_columns}")
    
    # usdkrwì™€ ì™¸ìƒë³€ìˆ˜ë¥¼ ë³„ë„ë¡œ ì •ê·œí™”
    usdkrw_scaler = StandardScaler()
    exogenous_scaler = StandardScaler()
    
    # usdkrw ì •ê·œí™”
    df_train_scaled = df_train[feature_columns].copy()
    df_train_scaled['usdkrw(target)'] = usdkrw_scaler.fit_transform(df_train[['usdkrw(target)']])
    
    # ì™¸ìƒë³€ìˆ˜ ì •ê·œí™”
    exogenous_cols = [col for col in feature_columns if col != 'usdkrw(target)']
    df_train_scaled[exogenous_cols] = exogenous_scaler.fit_transform(df_train[exogenous_cols])
    
    # target ë°ì´í„°ëŠ” usdkrwëŠ” ì›ë³¸ ê°’ ìœ ì§€, ì™¸ìƒë³€ìˆ˜ë§Œ ì •ê·œí™”
    df_target_scaled = df_target[feature_columns].copy()
    # usdkrwëŠ” ì›ë³¸ ê°’ ìœ ì§€ (ì •ê·œí™”í•˜ì§€ ì•ŠìŒ)
    df_target_scaled[exogenous_cols] = exogenous_scaler.transform(df_target[exogenous_cols])
    
    return df_train_scaled, df_target_scaled, usdkrw_scaler, exogenous_scaler, feature_columns

def create_sequences(data, seq_length=30, pred_length=7):
    """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±"""
    sequences = []
    targets = []
    
    for i in range(seq_length, len(data) - pred_length + 1):
        # ì…ë ¥ ì‹œí€€ìŠ¤
        seq = data.iloc[i-seq_length:i].values
        # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ (ë¯¸ë˜ 7ì¼)
        target = data.iloc[i:i+pred_length, 0].values  # usdkrwë§Œ
        
        sequences.append(seq)
        targets.append(target)
    
    return np.array(sequences), np.array(targets)

# ============================================================================
# 3. ëª¨ë¸ í•™ìŠµ
# ============================================================================

def train_model(model, train_sequences, train_targets, epochs=100, learning_rate=0.001):
    """ëª¨ë¸ í•™ìŠµ"""
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    model = model.to(device)
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    
    # ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
    train_sequences_tensor = torch.FloatTensor(train_sequences).to(device)
    train_targets_tensor = torch.FloatTensor(train_targets).to(device)
    
    # í•™ìŠµ
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        
        # ì…ë ¥ ë°ì´í„° ë¶„ë¦¬
        endogenous_x = train_sequences_tensor[:, :, 0:1]  # usdkrwë§Œ
        exogenous_x = train_sequences_tensor[:, :, 1:]    # ì™¸ìƒë³€ìˆ˜ë“¤
        
        # ì˜ˆì¸¡
        outputs = model(endogenous_x, exogenous_x)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = criterion(outputs, train_targets_tensor)
        
        # ì—­ì „íŒŒ
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}")
    
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    return model

# ============================================================================
# 4. ì˜ˆì¸¡ ë° ê²°ê³¼ ë¶„ì„
# ============================================================================

def predict_future(model, last_sequence, usdkrw_scaler, feature_columns, days_to_predict=31, pred_length=7):
    """ë¯¸ë˜ ì˜ˆì¸¡"""
    print(f"ğŸ”® {days_to_predict}ì¼ ë¯¸ë˜ ì˜ˆì¸¡ ì¤‘...")
    
    device = next(model.parameters()).device
    model.eval()
    
    predictions = []
    current_sequence = last_sequence.copy()
    
    with torch.no_grad():
        for i in range(0, days_to_predict, pred_length):
            # í˜„ì¬ ì‹œí€€ìŠ¤ë¥¼ í…ì„œë¡œ ë³€í™˜
            seq_tensor = torch.FloatTensor(current_sequence).unsqueeze(0).to(device)
            
            # ì…ë ¥ ë°ì´í„° ë¶„ë¦¬
            endogenous_x = seq_tensor[:, :, 0:1]
            exogenous_x = seq_tensor[:, :, 1:]
            
            # ì˜ˆì¸¡
            pred = model(endogenous_x, exogenous_x)
            pred = pred.squeeze(0).cpu().numpy()
            
            # ì˜ˆì¸¡ê°’ì„ ì‹œí€€ìŠ¤ì— ì¶”ê°€
            for j, p in enumerate(pred):
                if i + j < days_to_predict:
                    new_row = current_sequence[-1].copy()
                    new_row[0] = p  # usdkrw ì˜ˆì¸¡ê°’
                    current_sequence = np.vstack([current_sequence, new_row])
                    predictions.append(p)
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ ìœ ì§€
            if len(current_sequence) > 30:
                current_sequence = current_sequence[-30:]
    
    # ì˜ˆì¸¡ê°’ ì—­ì •ê·œí™” (usdkrwë§Œ)
    predictions_rescaled = []
    for pred in predictions:
        # usdkrw ì˜ˆì¸¡ê°’ë§Œ ì—­ì •ê·œí™”
        pred_rescaled = usdkrw_scaler.inverse_transform([[pred]])[0, 0]
        predictions_rescaled.append(pred_rescaled)
    
    return predictions_rescaled

def plot_results(actual, predicted, dates):
    """ê²°ê³¼ ì‹œê°í™”"""
    plt.figure(figsize=(15, 8))
    
    # ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’
    plt.plot(dates, actual, 'b-', label='Actual Values', linewidth=2)
    plt.plot(dates, predicted, 'r--', label='Predicted Values', linewidth=2)
    
    plt.title('LSTM-based Model with TimeXer Ideas: USD/KRW Exchange Rate Prediction (January 2025)', fontsize=16)
    plt.xlabel('Date', fontsize=12)
    plt.ylabel('USD/KRW Exchange Rate', fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.savefig('lstm_timexer_style_prediction_results.png', dpi=300, bbox_inches='tight')
    plt.show()

def save_results(actual, predicted, dates):
    """ê²°ê³¼ ì €ì¥"""
    results_df = pd.DataFrame({
        'date': dates,
        'actual': actual,
        'predicted': predicted,
        'error': np.array(actual) - np.array(predicted),
        'error_pct': ((np.array(actual) - np.array(predicted)) / np.array(actual)) * 100
    })
    
    results_df.to_csv('lstm_timexer_style_prediction_results.csv', index=False)
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    mse = mean_squared_error(actual, predicted)
    mae = mean_absolute_error(actual, predicted)
    rmse = np.sqrt(mse)
    
    print(f"\nğŸ“Š ì˜ˆì¸¡ ì„±ëŠ¥:")
    print(f"MSE: {mse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")
    
    # ìš”ì•½ í†µê³„ ì €ì¥
    summary = {
        'MSE': mse,
        'MAE': mae,
        'RMSE': rmse,
        'prediction_days': len(predicted),
        'model': 'Simple TimeXer'
    }
    
    summary_df = pd.DataFrame([summary])
    summary_df.to_csv('lstm_timexer_style_summary.csv', index=False)
    
    print(f"\nğŸ’¾ Results saved:")
    print(f"- Prediction results: lstm_timexer_style_prediction_results.csv")
    print(f"- Summary statistics: lstm_timexer_style_summary.csv")
    print(f"- Visualization: lstm_timexer_style_prediction_results.png")

# ============================================================================
# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 70)
    print("ğŸš€ LSTM-based Model with TimeXer Ideas: Training on df_trial.csv and Predicting target_trial.csv")
    print("=" * 70)
    
    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    df_train_scaled, df_target_scaled, usdkrw_scaler, exogenous_scaler, feature_columns = load_and_preprocess_data()
    
    # 2. ì‹œí€€ìŠ¤ ìƒì„±
    seq_length = 30
    pred_length = 7
    train_sequences, train_targets = create_sequences(df_train_scaled, seq_length, pred_length)
    print(f"ìƒì„±ëœ í•™ìŠµ ì‹œí€€ìŠ¤: {train_sequences.shape}")
    print(f"ìƒì„±ëœ íƒ€ê²Ÿ: {train_targets.shape}")
    
    # 3. ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    model = SimpleTimeXerModel(
        endogenous_dim=1,
        exogenous_dim=5,
        hidden_size=128,
        num_layers=2,
        output_size=pred_length
    )
    
    model = train_model(model, train_sequences, train_targets, epochs=100, learning_rate=0.001)
    
    # 4. ë¯¸ë˜ ì˜ˆì¸¡
    last_sequence = df_train_scaled.iloc[-seq_length:].values
    predictions = predict_future(model, last_sequence, usdkrw_scaler, feature_columns, days_to_predict=31, pred_length=pred_length)
    
    # 5. ê²°ê³¼ ë¶„ì„
    actual_values = df_target_scaled['usdkrw(target)'].values
    actual_dates = pd.date_range('2025-01-01', '2025-01-31', freq='D')
    
    print(f"\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
    print(f"ì‹¤ì œê°’ ë²”ìœ„: {actual_values.min():.1f} ~ {actual_values.max():.1f}")
    print(f"ì˜ˆì¸¡ê°’ ë²”ìœ„: {min(predictions):.1f} ~ {max(predictions):.1f}")
    
    # 6. ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥
    plot_results(actual_values, predictions, actual_dates)
    save_results(actual_values, predictions, actual_dates)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ LSTM-based Model with TimeXer Ideas Prediction Completed!")
    print("=" * 70)

if __name__ == "__main__":
    main() 